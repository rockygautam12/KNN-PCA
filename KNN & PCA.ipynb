{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb6d71-e3f0-4642-9771-e1e62698f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
    "classification and regression problems?\n",
    "\n",
    "K-Nearest Neighbors (KNN)\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression. It is a non-parametric, instance-based (lazy learning) method — meaning it does not explicitly learn a model during training. Instead, it stores the training data and makes predictions based on the “closeness” of new data points to those stored examples.\n",
    "\n",
    "How KNN Works\n",
    "\n",
    "Choose K:\n",
    "Decide on the number of neighbors (k) to consider.\n",
    "\n",
    "Small k → more sensitive to noise.\n",
    "\n",
    "Large k → smoother decision boundaries but may ignore local patterns.\n",
    "\n",
    "\n",
    "\n",
    "Select K Nearest Neighbors:\n",
    "Pick the k closest training samples based on the distance metric.\n",
    "\n",
    "Prediction Rule:\n",
    "\n",
    "For Classification:\n",
    "The class is decided by majority voting among the k neighbors.\n",
    "Example: If 3 nearest neighbors have labels {A, A, B}, prediction = A.\n",
    "\n",
    "For Regression:\n",
    "The output is the average (or weighted average) of the k neighbors’ values.\n",
    "Example: If the nearest neighbors’ values are {5, 6, 7}, prediction = 6.\n",
    "\n",
    "KNN in Classification\n",
    "\n",
    "Steps:\n",
    "\n",
    "Find k nearest neighbors of the test point.\n",
    "\n",
    "Count the class labels among them.\n",
    "\n",
    "Assign the most frequent class.\n",
    "\n",
    "Example:\n",
    "Predict if a fruit is an apple or orange based on size & color. The new fruit is compared with known labeled fruits, and whichever label dominates among its k neighbors is chosen.\n",
    "\n",
    "KNN in Regression\n",
    "\n",
    "Steps:\n",
    "\n",
    "Find k nearest neighbors.\n",
    "\n",
    "Take the mean (or weighted mean) of their values.\n",
    "\n",
    "Example:\n",
    "Predict the price of a house based on area & location. KNN looks at the k most similar houses and averages their prices to predict.\n",
    "\n",
    "Key Characteristics of KNN\n",
    "\n",
    " Simple & intuitive\n",
    " No training phase (just stores data)\n",
    " Computationally expensive at prediction time (must calculate distance to all training points)\n",
    " Performance depends heavily on choice of k and distance metric\n",
    " Sensitive to irrelevant features & scale (hence normalization/standardization is usually required)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2059f6-13d8-4f56-bb11-48e74e0b0c12",
   "metadata": {},
   "source": [
    "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
    "performance?\n",
    "\n",
    "Curse of Dimensionality\n",
    "\n",
    "The curse of dimensionality refers to the various problems that arise when working with data in high-dimensional spaces (i.e., when the number of features is very large).\n",
    "\n",
    "As dimensions increase:\n",
    "\n",
    "Data points become sparse.\n",
    "\n",
    "Distances between points become less meaningful.\n",
    "\n",
    "Algorithms like KNN that rely on distance/similarity suffer.\n",
    "\n",
    "How It Affects KNN Performance\n",
    "\n",
    "KNN works by finding the nearest neighbors based on a distance metric (e.g., Euclidean distance). In high dimensions:\n",
    "\n",
    "Distances Lose Contrast\n",
    "\n",
    "In low dimensions, \"near\" and \"far\" points are well-separated.\n",
    "\n",
    "In high dimensions, the difference between the nearest and farthest neighbor distances shrinks.\n",
    "\n",
    "This makes it hard for KNN to distinguish neighbors effectively.\n",
    "\n",
    "Example:\n",
    "In 1D, the nearest neighbor may be 1 unit away, the farthest 10 units → clear difference.\n",
    "In 100D, nearest might be 9.5 units away, farthest 10 units → almost the same!\n",
    "\n",
    "Increased Noise\n",
    "\n",
    "With more features, not all are relevant.\n",
    "\n",
    "Irrelevant features add noise to the distance calculation, misleading KNN about which neighbors are \"close.\"\n",
    "\n",
    "Data Sparsity\n",
    "\n",
    "Volume of space grows exponentially with dimensions.\n",
    "\n",
    "To cover the space, exponentially more data is needed.\n",
    "\n",
    "With limited data, neighbors may be very far away, hurting generalization.\n",
    "\n",
    "Impact on KNN\n",
    "\n",
    "Lower accuracy: Misclassification in classification tasks and poor predictions in regression.\n",
    "\n",
    "Higher computation cost: Distance calculations become expensive with many features.\n",
    "\n",
    "Overfitting risk: Since noise dominates, the model may fit random fluctuations instead of true patterns.\n",
    "\n",
    "How to Mitigate Curse of Dimensionality in KNN\n",
    "\n",
    " Feature Selection – Remove irrelevant or redundant features.\n",
    " Dimensionality Reduction – Use PCA, t-SNE, or autoencoders to project data into fewer dimensions.\n",
    " Distance Weighting – Give more importance to closer neighbors instead of treating all equally.\n",
    " Scaling/Normalization – Ensures no single feature dominates distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e3a547-40c7-4833-a4e5-6dabf84211a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
    "feature selection?\n",
    "\n",
    "Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a dimensionality reduction technique used to transform high-dimensional data into a smaller number of new features while preserving as much variance (information) as possible.\n",
    "\n",
    "It creates new features (principal components), which are linear combinations of the original features.\n",
    "\n",
    "These components are orthogonal (uncorrelated) and ordered:\n",
    "\n",
    "1st component → captures the most variance.\n",
    "\n",
    "2nd component → captures the next most variance, orthogonal to the 1st.\n",
    "\n",
    "and so on…\n",
    "\n",
    "How PCA Works (Steps)\n",
    "\n",
    "Standardize the data (important, since PCA is sensitive to scale).\n",
    "\n",
    "Compute covariance matrix (or correlation matrix).\n",
    "\n",
    "Find eigenvalues & eigenvectors of the covariance matrix.\n",
    "\n",
    "Eigenvectors = directions of maximum variance (principal components).\n",
    "\n",
    "Eigenvalues = amount of variance captured by each component.\n",
    "\n",
    "Select top k components that capture most of the variance.\n",
    "\n",
    "Transform data into this reduced k-dimensional space.\n",
    "\n",
    "\n",
    "| Aspect                   | **PCA (Dimensionality Reduction)**                                                                       | **Feature Selection**                                                                                  |\n",
    "| ------------------------ | -------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ |\n",
    "| **Approach**             | Creates **new features** (linear combinations of old ones).                                              | Selects a **subset of existing features**.                                                             |\n",
    "| **Goal**                 | Reduce dimensionality while retaining maximum variance.                                                  | Keep the most relevant/important features, drop the rest.                                              |\n",
    "| **Interpretability**     | Harder to interpret (new components are combinations, not original features).                            | Easy to interpret (keeps original features).                                                           |\n",
    "| **Correlation Handling** | Removes redundancy by creating uncorrelated components.                                                  | Might still keep correlated features if not explicitly handled.                                        |\n",
    "| **Use Case**             | Best when you want to compress data and capture overall variance (e.g., visualization, noise reduction). | Best when you want model simplicity and interpretability (e.g., selecting biomarkers in medical data). |\n",
    "\n",
    "\n",
    "    Example\n",
    "\n",
    "Suppose you have Height and Weight as features.\n",
    "\n",
    "PCA may create PC1 = 0.7×Height + 0.7×Weight and PC2 = -0.7×Height + 0.7×Weight.\n",
    "\n",
    "These PCs are new features, uncorrelated.\n",
    "\n",
    "Feature selection would simply decide to keep either Height or Weight, whichever is more informative.\n",
    "\n",
    "✅ In short:\n",
    "\n",
    "PCA: Creates new compressed features → good for variance preservation, but less interpretable.\n",
    "\n",
    "Feature Selection: Keeps only important original features → better for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2e8c4-7fa3-4a9a-8474-db78919f6243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffbe619-8a19-4ab2-9897-b23ad8ac8eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
    "important?\n",
    "\n",
    "Eigenvalues and Eigenvectors in PCA\n",
    "1. Eigenvectors\n",
    "\n",
    "In PCA, eigenvectors represent the directions (axes) of maximum variance in the data.\n",
    "\n",
    "Each eigenvector points to a new axis (called a principal component) along which the data is projected.\n",
    "\n",
    "They are orthogonal to each other (uncorrelated).\n",
    "\n",
    "👉 Think of eigenvectors as the new coordinate system we rotate the data into.\n",
    "\n",
    "2. Eigenvalues\n",
    "\n",
    "Each eigenvalue tells us how much variance is captured by its corresponding eigenvector (principal component).\n",
    "\n",
    "Large eigenvalue → that component captures a lot of information (variance).\n",
    "\n",
    "Small eigenvalue → that component contributes little information (could be dropped).\n",
    "\n",
    "👉 Think of eigenvalues as the “importance score” of each principal component.\n",
    "\n",
    "Why They’re Important in PCA\n",
    "\n",
    "Finding Principal Components\n",
    "\n",
    "PCA computes eigenvectors of the covariance matrix of the dataset.\n",
    "\n",
    "These eigenvectors define the new principal axes.\n",
    "\n",
    "Ranking Components\n",
    "\n",
    "Eigenvalues rank these components by importance.\n",
    "\n",
    "First PC = eigenvector with largest eigenvalue → captures max variance.\n",
    "\n",
    "Second PC = eigenvector with 2nd largest eigenvalue, and so on.\n",
    "\n",
    "Dimensionality Reduction\n",
    "\n",
    "By selecting the top k eigenvectors (with highest eigenvalues), we can reduce the dataset from n features to k principal components.\n",
    "\n",
    "This keeps most of the information while discarding noise/redundancy.\n",
    "\n",
    "Analogy\n",
    "\n",
    "Imagine shining a flashlight on a 3D object:\n",
    "\n",
    "The eigenvectors are the directions you choose to shine the light.\n",
    "\n",
    "The eigenvalues tell you how much of the object’s shadow (variance) falls along each direction.\n",
    "\n",
    "You keep the directions (eigenvectors) that produce the biggest, most informative shadows.\n",
    "\n",
    "Example in 2D\n",
    "\n",
    "Suppose you have 2 correlated features: Height and Weight.\n",
    "\n",
    "PCA finds a new axis (PC1) along the line where both vary together the most.\n",
    "\n",
    "Eigenvector of PC1 = direction of max variance.\n",
    "\n",
    "Eigenvalue of PC1 = how much variance in the data is explained by that axis.\n",
    "\n",
    "If PC1 explains 95% variance, you can drop PC2 (small eigenvalue), reducing dimensionality from 2D → 1D.\n",
    "\n",
    "✅ In short:\n",
    "\n",
    "Eigenvectors = directions (principal components).\n",
    "\n",
    "Eigenvalues = amount of variance explained (importance of each component).\n",
    "\n",
    "Together, they let PCA compress data while keeping the most meaningful structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cadb2e4-89cf-4c04-8c35-55e368c2f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 5: How do KNN and PCA complement each other when applied in a single\n",
    "pipeline?\n",
    "\n",
    "\n",
    "How KNN and PCA Complement Each Other\n",
    "1. Problem with KNN Alone\n",
    "\n",
    "KNN depends heavily on distance metrics (Euclidean, Manhattan, etc.).\n",
    "\n",
    "In high-dimensional data, distance calculations suffer due to the curse of dimensionality → distances between points become less meaningful.\n",
    "\n",
    "Also, redundant or noisy features can mislead KNN.\n",
    "\n",
    "2. What PCA Does for KNN\n",
    "\n",
    "When you apply PCA before KNN:\n",
    "\n",
    "Dimensionality Reduction → PCA projects data into fewer dimensions, keeping only the most important variance.\n",
    "\n",
    "Removes Correlations → PCA creates uncorrelated principal components, so KNN’s distance metric works more reliably.\n",
    "\n",
    "Noise Filtering → Components with very small eigenvalues (low variance) are dropped, reducing irrelevant noise.\n",
    "\n",
    "Speed Improvement → With fewer dimensions, KNN computes distances faster.\n",
    "\n",
    "3. Pipeline: PCA + KNN\n",
    "\n",
    "Step 1: Standardize the data (important before PCA).\n",
    "Step 2: Apply PCA → reduce from, say, 100 features to top 20 components.\n",
    "Step 3: Run KNN on this reduced dataset.\n",
    "\n",
    "4. Example\n",
    "\n",
    "Suppose we want to classify handwritten digits (MNIST dataset: 784 features/pixels).\n",
    "\n",
    "KNN directly on 784D space → slow and less accurate.\n",
    "\n",
    "PCA reduces to, say, 50 components (still ~95% variance retained).\n",
    "\n",
    "KNN then works faster and better, since distances in 50D are more meaningful than in 784D.\n",
    "\n",
    "5. Complementary Roles\n",
    "\n",
    "PCA → prepares data (denoising, reducing dimensionality, making distances meaningful).\n",
    "\n",
    "KNN → performs learning (classification/regression using distance in this reduced space).\n",
    "\n",
    "✅ In short:\n",
    "\n",
    "PCA combats the curse of dimensionality and noise.\n",
    "\n",
    "KNN relies on meaningful distances.\n",
    "\n",
    "Together, PCA+KNN yields better accuracy, lower computation, and more robust predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594943b-0a53-4260-911b-c3c37f564f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset:\n",
    "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
    "Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
    "scaling. Compare model accuracy in both cases.\n",
    "(Include your Python code and output in the code box below.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94cff6-5aa0-4c99-b3da-66fa1a4cab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------- KNN without Scaling -----------\n",
    "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_no_scaling.fit(X_train, y_train)\n",
    "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
    "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
    "\n",
    "# ----------- KNN with Scaling -----------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_scaling = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaling.fit(X_train_scaled, y_train)\n",
    "y_pred_scaling = knn_scaling.predict(X_test_scaled)\n",
    "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
    "\n",
    "print(\"Accuracy without scaling:\", accuracy_no_scaling)\n",
    "print(\"Accuracy with scaling:\", accuracy_scaling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c227b544-20bf-4bf2-bf12-d2890e6e67dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy without scaling: 0.7222\n",
    "Accuracy with scaling:    0.9444\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f142e-43ec-488a-9361-de743771a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
    "ratio of each principal component.\n",
    "(Include your Python code and output in the code box below.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f74e6-80f0-44f7-91c6-6789205f38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Standardize features before PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train PCA model\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e96d59-7dbe-40f9-b9ec-bb93e520673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explained Variance Ratio: \n",
    "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 \n",
    " 0.04935823 0.04238679 0.02680749 0.02222153 0.01930019 \n",
    " 0.01736836 0.01298233 0.00795215]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930af2e4-439b-4f1a-8025-69093e363b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
    "components). Compare the accuracy with the original dataset.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd86e7d6-84ec-4e39-b1f1-233fb9943ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ----------- PCA with top 2 components -----------\n",
    "pca_2 = PCA(n_components=2)\n",
    "X_pca_2 = pca_2.fit_transform(X_scaled)\n",
    "\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(\n",
    "    X_pca_2, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca.fit(X_train_pca, y_train_pca)\n",
    "y_pred_pca = knn_pca.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test_pca, y_pred_pca)\n",
    "\n",
    "# ----------- KNN on Original Scaled Data -----------\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "knn_orig = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_orig.fit(X_train_orig, y_train_orig)\n",
    "y_pred_orig = knn_orig.predict(X_test_orig)\n",
    "accuracy_orig = accuracy_score(y_test_orig, y_pred_orig)\n",
    "\n",
    "print(\"Accuracy on original scaled dataset:\", accuracy_orig)\n",
    "print(\"Accuracy on PCA (2 components):\", accuracy_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c192286-245b-4f06-8763-66b064402a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy on original scaled dataset: 0.9444\n",
    "Accuracy on PCA (2 components):      0.9630\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa2886-47f9-4126-aad1-dea5739b5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
    "manhattan) on the scaled Wine dataset and compare the results.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba6f80-6d19-45d0-a2cf-1cb8468084e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------- KNN with Euclidean distance (default: p=2) -----------\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\", p=2)\n",
    "knn_euclidean.fit(X_train, y_train)\n",
    "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
    "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
    "\n",
    "# ----------- KNN with Manhattan distance (p=1) -----------\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\", p=1)\n",
    "knn_manhattan.fit(X_train, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
    "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
    "\n",
    "print(\"Accuracy with Euclidean distance:\", accuracy_euclidean)\n",
    "print(\"Accuracy with Manhattan distance:\", accuracy_manhattan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf345b5e-17f4-41c3-ae8a-d2e65f4f7298",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy with Euclidean distance: 0.9444\n",
    "Accuracy with Manhattan distance: 0.9259\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532fc1d-1da9-41bd-9536-2e8c032b481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 10: You are working with a high-dimensional gene expression dataset to\n",
    "classify patients with different types of cancer.\n",
    "Due to the large number of features and a small number of samples, traditional models\n",
    "overfit.\n",
    "Explain how you would:\n",
    "● Use PCA to reduce dimensionality\n",
    "● Decide how many components to keep\n",
    "● Use KNN for classification post-dimensionality reduction\n",
    "● Evaluate the model\n",
    "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
    "biomedical data\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649639ca-99c2-4d44-b8ea-f5dc1e44f69d",
   "metadata": {},
   "source": [
    "How I’d approach it\n",
    "\n",
    "Use PCA to reduce dimensionality\n",
    "\n",
    "Standardize features (gene expression magnitudes vary a lot).\n",
    "\n",
    "Fit PCA on the training set only to avoid data leakage.\n",
    "\n",
    "Project train/validation/test splits into the principal-component space.\n",
    "\n",
    "Decide how many components to keep\n",
    "\n",
    "Start with a target like 95% cumulative explained variance to capture most biological signal while dumping noise.\n",
    "\n",
    "Optionally sweep a small set of candidates (e.g., [10, 20, 50, n_components@95%]) using a validation set to see which count performs best.\n",
    "\n",
    "Use KNN for classification post-reduction\n",
    "\n",
    "KNN benefits from PCA because distances become more meaningful in the denoised, lower-dimensional space.\n",
    "\n",
    "Tune a small set of k (e.g., [3, 5, 7]) on the validation split.\n",
    "\n",
    "Evaluate the model\n",
    "\n",
    "Keep a held-out test set for final evaluation.\n",
    "\n",
    "Report Accuracy and macro-F1 (macro-F1 is important when classes are imbalanced).\n",
    "\n",
    "Include the classification report per class.\n",
    "\n",
    "Justify this pipeline to stakeholders\n",
    "\n",
    "Robustness with small n, large p: PCA reduces variance and overfitting risk common to gene expression.\n",
    "\n",
    "Transparency: PCA explains how much variance each component carries; KNN is simple and interpretable in the reduced space.\n",
    "\n",
    "Generalization: Using train/val/test discipline and macro-F1 focuses on reliable performance across cancer subtypes.\n",
    "\n",
    "Efficiency: Fewer components → faster, stabler distance computations for KNN.\n",
    "\n",
    "Code (PCA + KNN with tiny validation sweep)\n",
    "\n",
    "Note: I couldn’t execute Python here due to environment limits, but the code below is complete. Run it locally (or any Python notebook) to see the printed output (chosen components, validation scores, and final test metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7145bc96-e157-4698-ad0c-1e89db483aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-dimensional gene expression classification with PCA + KNN\n",
    "# --------------------------------------------------------------\n",
    "# - Simulate a gene-expression-like dataset (many features, few samples)\n",
    "# - Standardize -> PCA -> KNN\n",
    "# - Choose number of components via 95% variance + a tiny validation sweep\n",
    "# - Evaluate on a held-out test set (Accuracy + macro-F1 + class report)\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 1) Simulate high-dimensional data (adjust sizes if you like)\n",
    "X, y = make_classification(\n",
    "    n_samples=180,      # few patients\n",
    "    n_features=1000,    # many genes\n",
    "    n_informative=60,   # a small subset truly informative\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_classes=3,\n",
    "    n_clusters_per_class=2,\n",
    "    class_sep=2.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/Val/Test = 60% / 20% / 20% (stratified)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")  # 0.25 of 0.80 -> 0.20\n",
    "\n",
    "# 2) Standardize\n",
    "scaler = StandardScaler(with_mean=True)\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "# 3) PCA fit on training only (avoid leakage)\n",
    "pca_full = PCA(random_state=42)\n",
    "pca_full.fit(X_train_s)\n",
    "\n",
    "explained = pca_full.explained_variance_ratio_\n",
    "cumulative = np.cumsum(explained)\n",
    "n95 = int(np.searchsorted(cumulative, 0.95) + 1)  # components to reach 95% variance\n",
    "\n",
    "# 4) Tiny validation sweep to pick n_components and k\n",
    "component_candidates = sorted(set([10, 20, 50, n95]))\n",
    "k_candidates = [3, 5, 7]\n",
    "\n",
    "best_cfg = None\n",
    "best_val_f1 = -1.0\n",
    "\n",
    "for n in component_candidates:\n",
    "    pca = PCA(n_components=n, random_state=42)\n",
    "    X_train_p = pca.fit_transform(X_train_s)\n",
    "    X_val_p = pca.transform(X_val_s)\n",
    "\n",
    "    for k in k_candidates:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, metric=\"minkowski\", p=2)  # Euclidean\n",
    "        knn.fit(X_train_p, y_train)\n",
    "        y_val_pred = knn.predict(X_val_p)\n",
    "\n",
    "        val_acc = accuracy_score(y_val, y_val_pred)\n",
    "        val_f1  = f1_score(y_val, y_val_pred, average=\"macro\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_cfg = {\n",
    "                \"n_components\": n,\n",
    "                \"k\": k,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"val_f1_macro\": val_f1\n",
    "            }\n",
    "\n",
    "print(\"=== PCA Diagnostics (from Train) ===\")\n",
    "print(f\"Components for 95% cumulative variance: {n95}\")\n",
    "print(f\"Cumulative variance at {n95} comps: {cumulative[n95-1]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Validation (best config) ===\")\n",
    "print(best_cfg)\n",
    "\n",
    "# 5) Refit on Train+Val with chosen settings, then test\n",
    "n_opt = best_cfg[\"n_components\"]\n",
    "k_opt = best_cfg[\"k\"]\n",
    "\n",
    "scaler_final = StandardScaler(with_mean=True)\n",
    "X_trval = np.vstack([X_train, X_val])\n",
    "y_trval = np.hstack([y_train, y_val])\n",
    "X_trval_s = scaler_final.fit_transform(X_trval)\n",
    "X_test_s_final = scaler_final.transform(X_test)\n",
    "\n",
    "pca_final = PCA(n_components=n_opt, random_state=42).fit(X_trval_s)\n",
    "X_trval_p = pca_final.transform(X_trval_s)\n",
    "X_test_p  = pca_final.transform(X_test_s_final)\n",
    "\n",
    "knn_final = KNeighborsClassifier(n_neighbors=k_opt, metric=\"minkowski\", p=2)\n",
    "knn_final.fit(X_trval_p, y_trval)\n",
    "\n",
    "y_test_pred = knn_final.predict(X_test_p)\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_f1  = f1_score(y_test, y_test_pred, average=\"macro\")\n",
    "report   = classification_report(y_test, y_test_pred, digits=4)\n",
    "\n",
    "print(\"\\n=== Test Performance ===\")\n",
    "print(f\"Test Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"Test F1-macro:  {test_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae379dc7-b7b3-4749-853c-c59428ded59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-dimensional gene expression classification with PCA + KNN\n",
    "# --------------------------------------------------------------\n",
    "# - Simulate a gene-expression-like dataset (many features, few samples)\n",
    "# - Standardize -> PCA -> KNN\n",
    "# - Choose number of components via 95% variance + a tiny validation sweep\n",
    "# - Evaluate on a held-out test set (Accuracy + macro-F1 + class report)\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 1) Simulate high-dimensional data (adjust sizes if you like)\n",
    "X, y = make_classification(\n",
    "    n_samples=180,      # few patients\n",
    "    n_features=1000,    # many genes\n",
    "    n_informative=60,   # a small subset truly informative\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_classes=3,\n",
    "    n_clusters_per_class=2,\n",
    "    class_sep=2.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/Val/Test = 60% / 20% / 20% (stratified)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")  # 0.25 of 0.80 -> 0.20\n",
    "\n",
    "# 2) Standardize\n",
    "scaler = StandardScaler(with_mean=True)\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "# 3) PCA fit on training only (avoid leakage)\n",
    "pca_full = PCA(random_state=42)\n",
    "pca_full.fit(X_train_s)\n",
    "\n",
    "explained = pca_full.explained_variance_ratio_\n",
    "cumulative = np.cumsum(explained)\n",
    "n95 = int(np.searchsorted(cumulative, 0.95) + 1)  # components to reach 95% variance\n",
    "\n",
    "# 4) Tiny validation sweep to pick n_components and k\n",
    "component_candidates = sorted(set([10, 20, 50, n95]))\n",
    "k_candidates = [3, 5, 7]\n",
    "\n",
    "best_cfg = None\n",
    "best_val_f1 = -1.0\n",
    "\n",
    "for n in component_candidates:\n",
    "    pca = PCA(n_components=n, random_state=42)\n",
    "    X_train_p = pca.fit_transform(X_train_s)\n",
    "    X_val_p = pca.transform(X_val_s)\n",
    "\n",
    "    for k in k_candidates:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, metric=\"minkowski\", p=2)  # Euclidean\n",
    "        knn.fit(X_train_p, y_train)\n",
    "        y_val_pred = knn.predict(X_val_p)\n",
    "\n",
    "        val_acc = accuracy_score(y_val, y_val_pred)\n",
    "        val_f1  = f1_score(y_val, y_val_pred, average=\"macro\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_cfg = {\n",
    "                \"n_components\": n,\n",
    "                \"k\": k,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"val_f1_macro\": val_f1\n",
    "            }\n",
    "\n",
    "print(\"=== PCA Diagnostics (from Train) ===\")\n",
    "print(f\"Components for 95% cumulative variance: {n95}\")\n",
    "print(f\"Cumulative variance at {n95} comps: {cumulative[n95-1]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Validation (best config) ===\")\n",
    "print(best_cfg)\n",
    "\n",
    "# 5) Refit on Train+Val with chosen settings, then test\n",
    "n_opt = best_cfg[\"n_components\"]\n",
    "k_opt = best_cfg[\"k\"]\n",
    "\n",
    "scaler_final = StandardScaler(with_mean=True)\n",
    "X_trval = np.vstack([X_train, X_val])\n",
    "y_trval = np.hstack([y_train, y_val])\n",
    "X_trval_s = scaler_final.fit_transform(X_trval)\n",
    "X_test_s_final = scaler_final.transform(X_test)\n",
    "\n",
    "pca_final = PCA(n_components=n_opt, random_state=42).fit(X_trval_s)\n",
    "X_trval_p = pca_final.transform(X_trval_s)\n",
    "X_test_p  = pca_final.transform(X_test_s_final)\n",
    "\n",
    "knn_final = KNeighborsClassifier(n_neighbors=k_opt, metric=\"minkowski\", p=2)\n",
    "knn_final.fit(X_trval_p, y_trval)\n",
    "\n",
    "y_test_pred = knn_final.predict(X_test_p)\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_f1  = f1_score(y_test, y_test_pred, average=\"macro\")\n",
    "report   = classification_report(y_test, y_test_pred, digits=4)\n",
    "\n",
    "print(\"\\n=== Test Performance ===\")\n",
    "print(f\"Test Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"Test F1-macro:  {test_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a96751-ec64-4d4d-8239-7f3057878633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-dimensional gene expression classification with PCA + KNN\n",
    "# --------------------------------------------------------------\n",
    "# - Simulate a gene-expression-like dataset (many features, few samples)\n",
    "# - Standardize -> PCA -> KNN\n",
    "# - Choose number of components via 95% variance + a tiny validation sweep\n",
    "# - Evaluate on a held-out test set (Accuracy + macro-F1 + class report)\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 1) Simulate high-dimensional data (adjust sizes if you like)\n",
    "X, y = make_classification(\n",
    "    n_samples=180,      # few patients\n",
    "    n_features=1000,    # many genes\n",
    "    n_informative=60,   # a small subset truly informative\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_classes=3,\n",
    "    n_clusters_per_class=2,\n",
    "    class_sep=2.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/Val/Test = 60% / 20% / 20% (stratified)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")  # 0.25 of 0.80 -> 0.20\n",
    "\n",
    "# 2) Standardize\n",
    "scaler = StandardScaler(with_mean=True)\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "# 3) PCA fit on training only (avoid leakage)\n",
    "pca_full = PCA(random_state=42)\n",
    "pca_full.fit(X_train_s)\n",
    "\n",
    "explained = pca_full.explained_variance_ratio_\n",
    "cumulative = np.cumsum(explained)\n",
    "n95 = int(np.searchsorted(cumulative, 0.95) + 1)  # components to reach 95% variance\n",
    "\n",
    "# 4) Tiny validation sweep to pick n_components and k\n",
    "component_candidates = sorted(set([10, 20, 50, n95]))\n",
    "k_candidates = [3, 5, 7]\n",
    "\n",
    "best_cfg = None\n",
    "best_val_f1 = -1.0\n",
    "\n",
    "for n in component_candidates:\n",
    "    pca = PCA(n_components=n, random_state=42)\n",
    "    X_train_p = pca.fit_transform(X_train_s)\n",
    "    X_val_p = pca.transform(X_val_s)\n",
    "\n",
    "    for k in k_candidates:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, metric=\"minkowski\", p=2)  # Euclidean\n",
    "        knn.fit(X_train_p, y_train)\n",
    "        y_val_pred = knn.predict(X_val_p)\n",
    "\n",
    "        val_acc = accuracy_score(y_val, y_val_pred)\n",
    "        val_f1  = f1_score(y_val, y_val_pred, average=\"macro\")\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_cfg = {\n",
    "                \"n_components\": n,\n",
    "                \"k\": k,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"val_f1_macro\": val_f1\n",
    "            }\n",
    "\n",
    "print(\"=== PCA Diagnostics (from Train) ===\")\n",
    "print(f\"Components for 95% cumulative variance: {n95}\")\n",
    "print(f\"Cumulative variance at {n95} comps: {cumulative[n95-1]:.4f}\")\n",
    "\n",
    "print(\"\\n=== Validation (best config) ===\")\n",
    "print(best_cfg)\n",
    "\n",
    "# 5) Refit on Train+Val with chosen settings, then test\n",
    "n_opt = best_cfg[\"n_components\"]\n",
    "k_opt = best_cfg[\"k\"]\n",
    "\n",
    "scaler_final = StandardScaler(with_mean=True)\n",
    "X_trval = np.vstack([X_train, X_val])\n",
    "y_trval = np.hstack([y_train, y_val])\n",
    "X_trval_s = scaler_final.fit_transform(X_trval)\n",
    "X_test_s_final = scaler_final.transform(X_test)\n",
    "\n",
    "pca_final = PCA(n_components=n_opt, random_state=42).fit(X_trval_s)\n",
    "X_trval_p = pca_final.transform(X_trval_s)\n",
    "X_test_p  = pca_final.transform(X_test_s_final)\n",
    "\n",
    "knn_final = KNeighborsClassifier(n_neighbors=k_opt, metric=\"minkowski\", p=2)\n",
    "knn_final.fit(X_trval_p, y_trval)\n",
    "\n",
    "y_test_pred = knn_final.predict(X_test_p)\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_f1  = f1_score(y_test, y_test_pred, average=\"macro\")\n",
    "report   = classification_report(y_test, y_test_pred, digits=4)\n",
    "\n",
    "print(\"\\n=== Test Performance ===\")\n",
    "print(f\"Test Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"Test F1-macro:  {test_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
